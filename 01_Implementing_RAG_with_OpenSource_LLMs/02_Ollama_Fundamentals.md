# üß† Guida Completa a Ollama: Setup Locale, Scelta dei Modelli e Esecuzione

Questa lezione fornisce una panoramica pratica e approfondita sull‚Äôuso di **Ollama** per eseguire **modelli LLM localmente**, controllare l'hardware disponibile, selezionare modelli adatti e configurare un server accessibile da applicazioni esterne come AnythingLLM, Flowise o terminale.

---

## üì¶ 1. Installazione di Ollama

### ‚úÖ Procedura:

1. Vai su **[ollama.com](https://ollama.com)**
2. Clicca su **Download**
3. Scegli la versione per il tuo sistema operativo: **Windows / Mac / Linux**
4. Esegui l'installer (`.exe`, `.pkg`, ecc.)
5. Dopo l'installazione:

   * Ollama √® **attivo in background**
   * Verifica la presenza dell'icona in basso a destra (Windows)
   * Per riaprirlo, cerca *"Ollama"* nel menu Start

---

## üß™ 2. Verifica Hardware (VRAM)

Per eseguire un modello LLM localmente, √® importante verificare che la tua **VRAM** sia sufficiente.

### üîç Tool consigliato:

* **TechPowerUp GPU-Z** ([techpowerup.com](https://www.techpowerup.com/gpuz/))

  * Mostra memoria disponibile, tipo GPU e altre specifiche.

### üìå Regola generale:

> **VRAM ‚â• dimensione modello (in GB) √ó 1.1**

---

## üì• 3. Scaricare un Modello

### üîç Come trovare modelli:

1. Vai su [ollama.com/library](https://ollama.com/library)
2. Filtra per:

   * `Embeddings`, `Vision`, `Tools`, ecc.
   * Ordinamento: `Popular`, `Newest`
3. Seleziona il modello (es: `llama3`, `mistral`, `dolphin`, `gemma`)

### üß† Parametri importanti:

* **Parametri modello (es. 3B, 7B, 13B)** ‚Üí pi√π alto = pi√π intelligente (generalmente)
* **Formato:**

  * `FP16`: modello completo, pi√π preciso
  * `Q8`, `Q6`, `Q4`, `Q2`: versioni quantizzate ‚Üí meno VRAM, ma meno precisi

### üóÇ Esempio:

| Modello           | Dimensione | Formato | VRAM richiesta |
| ----------------- | ---------- | ------- | -------------- |
| LLaMA 3.2 3B Q4   | 2.1 GB     | Q4      | ‚â• 2.5 GB       |
| LLaMA 3.2 3B FP16 | 6.4 GB     | FP16    | ‚â• 7 GB         |

---

## üíª 4. Usare il Terminale con Ollama

### üìå Comandi fondamentali:

| Comando                 | Descrizione                                   |
| ----------------------- | --------------------------------------------- |
| `ollama pull modello`   | Scarica il modello desiderato                 |
| `ollama list`           | Elenca i modelli installati                   |
| `ollama run modello`    | Avvia un modello in modalit√† chat interattiva |
| `ollama rm modello`     | Rimuove un modello                            |
| `ollama show modello`   | Mostra tutte le caratteristiche del modello   |
| `ollama serve`          | Avvia il server API REST sulla porta 11434    |

---

## üåê 5. Avviare il Server

Dopo aver installato un modello, puoi esporre un'**API compatibile con OpenAI** in locale:

```bash
ollama serve
```

* **Porta predefinita**: `http://localhost:11434`
* Pu√≤ essere usata da:

  * AnythingLLM
  * Flowise
  * Postman
  * Terminale con `curl`

---

## üß™ 6. Esempio Pratico

1. Scarica un modello (es. LLaMA 3.2 3B Q4):

```bash
ollama pull llama3:3b-q4
```

2. Avvia il modello:

```bash
ollama run llama3:3b-q4
```

3. Chatta con il modello:

```bash
> Give me the code for snake
```

4. Avvia il server:

```bash
ollama serve
```

---

## üìö 7. Dolphin Models ‚Äì Modelli Uncensored

* Basati su LLaMA (es. `dolphin-llama3-8b`)
* **Non censurati**: rispondono anche a prompt sensibili o controversi
* Ideali per:

  * Coding
  * Math
  * Funzioni generiche
  * Evitare filtri imposti da provider cloud

---

## üìÅ 8. Info Modelli & Documentazione

Consulta il repository ufficiale:

* [GitHub - Ollama](https://github.com/ollama/ollama)

### Contenuti utili:

* Comandi CLI
* Documentazione Docker
* API Reference
* Modelli supportati
* Quantizzazione (`gguf` format)

---

## ‚úÖ Riepilogo Finale

| Step                   | Descrizione                             |
| ---------------------- | --------------------------------------- |
| ‚úÖ Installa Ollama      | Dal sito ufficiale                      |
| ‚úÖ Verifica la tua VRAM | Usa TechPowerUp GPU-Z                   |
| ‚úÖ Scegli un modello    | Usa FP16 per qualit√†, Q4 per leggerezza |
| ‚úÖ Scarica ed esegui    | `ollama pull`, `ollama run`             |
| ‚úÖ Avvia il server      | `ollama serve` per endpoint locale      |
| ‚úÖ Integra in altre app | AnythingLLM, Flowise, terminale, ecc.   |
